{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOghrCfzxWcql+Q+I0NIHAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angiellanos/MNIST_DF/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Se usan las funciones *_load_label* y *_load_img* para cargar el conjunto de datos del MNIST. Luego, se usa la función *train_test_split* de la librería *sklearn* para dividir el conjunto de datos en un conjunto de entrenamiento y uno de prueba, asegurándose de que ambos estén balanceados. Es decir, que tengan la misma proporción de imágenes para cada dígito. Se usa el parámetro *stratify* de la función *train_test_split* para lograr esto. El tamaño del conjunto de prueba es 10000, entonces el conjunto de prueba tiene 10000 imágenes y el conjunto de entrenamiento 50000 imágenes."
      ],
      "metadata": {
        "id": "JBZ_OklUjsek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! git clone https://github.com/angiellanos/MNIST_DF.git"
      ],
      "metadata": {
        "id": "oVcM3IK4ykVr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MNIST_DF/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D8K5fbcJ_eh",
        "outputId": "d13c87f1-ed8b-4adf-b855-74430f8ccfe8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MNIST_DF/MNIST_DF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZonNMmDKLSp",
        "outputId": "732e68c6-d78c-439a-f3a6-e8f93d7d90a3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mMNIST_DF\u001b[0m/    t10k-images-idx3-ubyte.gz   train-labels-idx1-ubyte.gz\n",
            "MNIST.ipynb  t10k-labels-idx1-ubyte.gz\n",
            "README.md    train-images-idx3-ubyte.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNBuis3fJBNr",
        "outputId": "99a25c59-543c-4e98-fc59-ea8b325e5476"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MNIST_DF/MNIST_DF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "files = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "image_size = 28\n",
        "num_images = [60000, 10000]\n",
        "num_labels = [60000, 10000]\n",
        "\n",
        "def _load_label(file_key):\n",
        "    \"\"\"Función que carga las etiquetas de las imágenes desde un archivo comprimido en formato gzip.\n",
        "    Args:\n",
        "        file_name (str): el nombre del archivo que contiene las etiquetas\n",
        "    Returns:\n",
        "        array: etiquetas de las imágenes.\n",
        "    \"\"\"\n",
        "    file_name = files[file_key]\n",
        "    file_path = os.getcwd() + \"/\" + file_name\n",
        "\n",
        "    j = 0 if file_key=='train_label' else 1\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        # omite los primeros 8 bytes\n",
        "        f.read(8)\n",
        "        buf = f.read(num_labels[j])\n",
        "        label = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
        "        # print(label[:20])\n",
        "\n",
        "    return label\n",
        "\n",
        "def _load_img(file_key):\n",
        "    \"\"\"Función que carga las imágenes desde un archivo comprimido en formato gzip.\n",
        "    Args:\n",
        "        file_name (str): el nombre del archivo que contiene las etiquetas\n",
        "    Returns:\n",
        "        array: imágenes en forma de vectores de tamaño 784.\n",
        "    \"\"\"\n",
        "    file_name = files[file_key]\n",
        "    file_path = os.getcwd() + \"/\" + file_name\n",
        "\n",
        "    j = 0 if file_key=='train_img' else 1\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        # omite los primeros 16 bytes\n",
        "        f.read(16)\n",
        "        # cada pixel en 1 byte = 8 bits\n",
        "        # lee todos los datos y los coloca en un buffer de memoria\n",
        "        buf = f.read(image_size * image_size * num_images[j])\n",
        "        # traslada los datos a un array de numpy de tipo float32\n",
        "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "        # Cambia la forma de los datos par entregarlos listos al dataset\n",
        "        data = data.reshape(num_images[j], image_size, image_size, 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "def _convert_numpy():\n",
        "    \"\"\"Función que convierte los datos del MNIST en arrays de NumPy y los guarda en dos diccionarios.\n",
        "    Args: None\n",
        "    Returns:\n",
        "        dictionary: con las claves ‘train_img’, ‘train_label’, ‘test_img’ y ‘test_label’,\n",
        "                   y los valores correspondientes a los arrays de NumPy con las imágenes\n",
        "                   y las etiquetas.\n",
        "    \"\"\"\n",
        "    # Cargar el conjunto de datos del MNIST usando las funciones _load_label y _load_img\n",
        "    X_train = _load_img('train_img')\n",
        "    y_train = _load_label('train_label')\n",
        "    X_test = _load_img('test_img')\n",
        "    y_test = _load_label('test_label')\n",
        "\n",
        "    # Imprimir las dimensiones de los conjuntos de datos\n",
        "    print(\"Dimensiones del conjunto de entrenamiento:\")\n",
        "    print(X_train.shape)\n",
        "    print(y_train.shape)\n",
        "    print(\"Dimensiones del conjunto de prueba:\")\n",
        "    print(X_test.shape)\n",
        "    print(y_test.shape)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = _convert_numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "sXozfCHCGHcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf9f592-da4e-49e8-ee87-7e98ee0f9779"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones del conjunto de entrenamiento:\n",
            "(60000, 28, 28, 1)\n",
            "(60000,)\n",
            "Dimensiones del conjunto de prueba:\n",
            "(10000, 28, 28, 1)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez construidos los conjuntos de entrenamiento y prueba, se procede a construir el modelo de Naive Bayes. Para ello, se necesita calcular las probabilidades a priori de cada clase (P(c), donde c es la clase de cada dígito, p.e. la clase del 0, la del 1, hasta la del 9) y las probabilidades condicionales de cada píxel dado cada clase (P(x|c)). Estas probabilidades se pueden estimar a partir de los datos de entrenamiento usando la regla de Laplace, que consiste en añadir un pequeño valor (por ejemplo, 1) al numerador y al denominador de las frecuencias relativas.\n",
        "\n",
        "Por lo tanto, la probabilidad a priori de cada clase $α$ (P(c=$α$)) está dada por:"
      ],
      "metadata": {
        "id": "lcoScNh-3QSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar el número total de imágenes\n",
        "n_total = len(y_train)\n",
        "\n",
        "# Crear una lista vacía para guardar las probabilidades a priori\n",
        "p_c = []\n",
        "n_c = []\n",
        "# Recorrer las clases del 0 al 9\n",
        "for c in range(10):\n",
        "    # Contar el número de imágenes que tienen la etiqueta c\n",
        "    n_c.append(np.sum(y_train == c))\n",
        "    # Aplicar la regla de Laplace con un valor k=1\n",
        "    p_c.append((n_c[c] + 1) / (n_total + 10))\n",
        "\n",
        "# Convertir la lista en un array de numpy\n",
        "p_c = np.array(p_c)\n",
        "n_c = np.array(n_c)\n",
        "\n",
        "print(\"P(C=c)=\",p_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7pJlYbe4hDh",
        "outputId": "dbf7dd38-e4b4-43bf-d95a-85f3ca2319f4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(C=c)= [0.09871688 0.11236461 0.09930012 0.10218297 0.09736711 0.09035161\n",
            " 0.09863356 0.10441593 0.09751708 0.09915014]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas probabilidades siempre serán cercanas a 0.1, que es la probabilidad esperada de cada dígito, dada por:"
      ],
      "metadata": {
        "id": "5m8HQpD95NRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $P(C=c)=\\frac{Frecuencia \\ del \\ dígito}{Cantidad \\ de \\ dígitos}=\\frac{1}{10}=0.1$"
      ],
      "metadata": {
        "id": "xvRj1WKn7bWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a que son complementarias, su suma debe ser igual a 1. Verificando:"
      ],
      "metadata": {
        "id": "6YpJ18D68jD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Suma de las probabilidades a priori:\", np.sum(p_c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrFntiu08rV5",
        "outputId": "e8072fc8-e231-4514-d422-f1fee69feba3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suma de las probabilidades a priori: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para calcular las probabilidades condicionales de cada píxel dado cada clase (P(x|c)), se necesita contar el número de veces que cada píxel tiene un valor mayor que cero en las imágenes de cada clase. Así, se obtiene que:"
      ],
      "metadata": {
        "id": "dv5E4_ZU9A4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una lista vacía para guardar las probabilidades condicionales\n",
        "p_x_c = []\n",
        "# Recorrer las clases del 0 al 9\n",
        "for c in range(10):\n",
        "    # Seleccionar las imágenes que tienen la etiqueta c\n",
        "    X_c = X_train[np.where(y_train == c)]\n",
        "    # Crear una lista vacía para guardar las probabilidades condicionales de cada píxel dado la clase c\n",
        "    p_x_c_c = []\n",
        "    # Recorrer los píxeles del 1 al 784\n",
        "    for i in range(784):\n",
        "        # Calcular el índice de la fila y la columna del píxel i\n",
        "        row = i // 28\n",
        "        col = i % 28\n",
        "        # Contar el número de imágenes que tienen el píxel i mayor o igual que cero\n",
        "        n_xi_c = np.sum(X_c[:, row, col, 0] > 0.1)\n",
        "        # Aplicar la regla de Laplace con un valor k=1\n",
        "        p_xi_c = (n_xi_c + 1) / (n_c[c] + 2)\n",
        "        # Añadir la probabilidad condicional a la lista\n",
        "        p_x_c_c.append(p_xi_c)\n",
        "    # Convertir la lista en un array de numpy y añadirlo a la lista principal\n",
        "    p_x_c.append(np.array(p_x_c_c))\n",
        "\n",
        "# Convertir la lista principal en un array de numpy\n",
        "p_x_c = np.array(p_x_c)\n"
      ],
      "metadata": {
        "id": "Ax7nMDEk9AB4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que la suma de las probabilidades condicionales por clase no tiene que ser 1, porque son independientes entre sí. Es decir, los pixeles se toman independientes entre sí para cada dígito en la imagen."
      ],
      "metadata": {
        "id": "musCtfOtRG6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_clase = np.sum(p_x_c, axis=1)\n",
        "print(\"Suma de las probabilidades condicionales por clase:\", x_clase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UipQipDSRXuo",
        "outputId": "ef5dbce0-529e-47cb-ec7c-d6e6a101cd66"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suma de las probabilidades condicionales por clase: [192.04236287  85.93683274 168.8840604  163.411218   141.89253936\n",
            " 152.4182187  157.01435811 131.48045317 173.39039809 143.11645102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vAzGqicV1kIZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7XGv-0aFEdR4"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zfTvXu2qsVnc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_qY3PZccH4Gg"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFVaNtNxH6YY"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}