{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOjRAkaJ2xiuUOU2DmsK/QO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angiellanos/MNIST_DF/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Se usan las funciones *_load_label* y *_load_img* para cargar el conjunto de datos del MNIST. Luego, se usa la función *train_test_split* de la librería *sklearn* para dividir el conjunto de datos en un conjunto de entrenamiento y uno de prueba, asegurándose de que ambos estén balanceados. Es decir, que tengan la misma proporción de imágenes para cada dígito. Se usa el parámetro *stratify* de la función *train_test_split* para lograr esto. El tamaño del conjunto de prueba es 10000, entonces el conjunto de prueba tiene 10000 imágenes y el conjunto de entrenamiento 60000 imágenes."
      ],
      "metadata": {
        "id": "JBZ_OklUjsek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Rutas y nombres de los archivos que contienen los datos de MNIST\n",
        "route = 'http://yann.lecun.com/exdb/mnist/'\n",
        "files = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "# Directorio actual y nombre del archivo donde se guardó el conjunto de datos en formato pickle\n",
        "dataset_direct = os.getcwd()\n",
        "save_file = dataset_direct + \"/mnist.pkl\"\n",
        "\n",
        "image_size = 28\n",
        "num_images = [60000, 10000]\n",
        "num_labels = [60000, 10000]\n",
        "\n",
        "def _load_label(file_key):\n",
        "    \"\"\"Función que carga las etiquetas de las imágenes desde un archivo comprimido en formato gzip.\n",
        "    Args:\n",
        "        file_name (str): el nombre del archivo que contiene las etiquetas\n",
        "    Returns:\n",
        "        array: etiquetas de las imágenes.\n",
        "    \"\"\"\n",
        "    file_name = files[file_key]\n",
        "    file_path = dataset_direct + \"/\" + file_name\n",
        "\n",
        "    j = 0 if file_key=='train_label' else 1\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        # omite los primeros 8 bytes\n",
        "        f.read(8)\n",
        "        buf = f.read(num_labels[j])\n",
        "        label = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
        "        # print(label[:20])\n",
        "\n",
        "    return label\n",
        "\n",
        "def _load_img(file_key):\n",
        "    \"\"\"Función que carga las imágenes desde un archivo comprimido en formato gzip.\n",
        "    Args:\n",
        "        file_name (str): el nombre del archivo que contiene las etiquetas\n",
        "    Returns:\n",
        "        array: imágenes en forma de vectores de tamaño 784.\n",
        "    \"\"\"\n",
        "    file_name = files[file_key]\n",
        "    file_path = dataset_direct + \"/\" + file_name\n",
        "\n",
        "    j = 0 if file_key=='train_img' else 1\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        # omite los primeros 16 bytes\n",
        "        f.read(16)\n",
        "        # cada pixel en 1 byte = 8 bits\n",
        "        # lee todos los datos y los coloca en un buffer de memoria\n",
        "        buf = f.read(image_size * image_size * num_images[j])\n",
        "        # traslada los datos a un array de numpy de tipo float32\n",
        "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "        # Cambia la forma de los datos par entregarlos listos al dataset\n",
        "        data = data.reshape(num_images[j], image_size, image_size, 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, labels, images, transform=None, target_transform=None):\n",
        "        self.labels = labels\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label =  self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n",
        "def _convert_numpy():\n",
        "    \"\"\"Función que convierte los datos del MNIST en arrays de NumPy y los guarda en dos diccionarios.\n",
        "    Args: None\n",
        "    Returns:\n",
        "        dictionary: con las claves ‘train_img’, ‘train_label’, ‘test_img’ y ‘test_label’,\n",
        "                   y los valores correspondientes a los arrays de NumPy con las imágenes\n",
        "                   y las etiquetas.\n",
        "    \"\"\"\n",
        "    # Cargar el conjunto de datos del MNIST usando las funciones _load_label y _load_img\n",
        "    X_train = _load_img('train_img')\n",
        "    y_train = _load_label('train_label')\n",
        "    X_test = _load_img('test_img')\n",
        "    y_test = _load_label('test_label')\n",
        "\n",
        "    # Dividir el conjunto de datos en un conjunto de entrenamiento y uno de prueba, usando la función train_test_split\n",
        "    # Se usa el parámetro stratify para asegurar que ambos conjuntos estén balanceados\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=10000, stratify=y_train)\n",
        "\n",
        "    # Crear los datasets personalizados usando la clase ImageDataset\n",
        "    train_dataset = ImageDataset(y_train, X_train)\n",
        "    test_dataset = ImageDataset(y_test, X_test)\n",
        "\n",
        "    # Imprimir las dimensiones de los conjuntos de datos\n",
        "    print(\"Dimensiones del conjunto de entrenamiento:\")\n",
        "    print(X_train.shape)\n",
        "    print(y_train.shape)\n",
        "    print(\"Dimensiones del conjunto de prueba:\")\n",
        "    print(X_test.shape)\n",
        "    print(y_test.shape)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "train_dataset, test_dataset = _convert_numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Preparando los datos para entrenamiento con DataLoaders\n",
        "num_workers = torch.get_num_threads()//2\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers= num_workers)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(torch.get_num_threads())\n",
        "\n",
        "len(next(iter(test_dataloader)))\n",
        "\n",
        "\n",
        "## Iterando a lo largo del DataLoader\n",
        "\n",
        "# Despliega imagen y etiqueta\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f'Shape del lote de imágenes: {train_features.size()}')\n",
        "print(f'Shape del lote de etiquetas: {train_labels.size()}')\n",
        "img = train_features[0].squeeze()# primer elemento del batch\n",
        "# squeeze elimina ejes de tamaño 1.\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.title(label.numpy())\n",
        "plt.show()\n",
        "\n",
        "class Draw:\n",
        "    def __init__(self, label_map, images_iterator, cols=4, rows=4, figsize=(6,6)):\n",
        "        self.data = images_iterator\n",
        "        self.label_map = label_map\n",
        "        self.figsize=figsize\n",
        "        self.cols = cols\n",
        "        self.rows = rows\n",
        "        self.index = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.index >= len(self.data):\n",
        "            raise StopIteration\n",
        "\n",
        "        self.index += 1\n",
        "\n",
        "        img, label = next(self.data)\n",
        "        figure = plt.figure(figsize=self.figsize)\n",
        "        for i in range(1, self.cols*self.rows+1):\n",
        "            #sample_idx = torch.randint(len(training_data), size =(1,)).item()\n",
        "            #img, label = training_data[sample_idx]\n",
        "            figure.add_subplot(self.rows, self.cols, i)\n",
        "            #plt.title(self.label_map[label])\n",
        "            plt.axis('off')\n",
        "            plt.imshow(img[i].squeeze(), cmap='gray')\n",
        "            plt.title(label[i].numpy())\n",
        "        plt.show()\n",
        "        #return value\n",
        "\n",
        "label_map = {\n",
        "    0: 'camiseta',\n",
        "    1: \"Pantalones\",\n",
        "    2: \"Jersey\",\n",
        "    3: \"Vestido\",\n",
        "    4: \"Abrigo\",\n",
        "    5: \"Sandalia\",\n",
        "    6: \"Camisa\",\n",
        "    7: \"Tenis\",\n",
        "    8: \"Bolso\",\n",
        "    9: \"Botines\",\n",
        "}\n",
        "\n",
        "images = Draw(label_map, iter(train_dataloader))\n",
        "\n",
        "next(images)"
      ],
      "metadata": {
        "id": "sXozfCHCGHcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "ca54099e-b64d-4144-858d-0f0ec3f363b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cd9ad60a9196>\u001b[0m in \u001b[0;36m<cell line: 127>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-cd9ad60a9196>\u001b[0m in \u001b[0;36m_convert_numpy\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     \"\"\"\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Cargar el conjunto de datos del MNIST usando las funciones _load_label y _load_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_img'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_img'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-cd9ad60a9196>\u001b[0m in \u001b[0;36m_load_img\u001b[0;34m(file_key)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile_key\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'train_img'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# omite los primeros 16 bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train-images-idx3-ubyte.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zfTvXu2qsVnc"
      }
    }
  ]
}